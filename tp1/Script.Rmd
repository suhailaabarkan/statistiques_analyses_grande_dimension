---
title: "Script"
output: html_document
date: "2024-09-21"
---

# Données

Ici on va se concentrer sur les données de malbouffe car il y a dedans des variables qualitatives et quantitatives.

```{r}
# Charger les données
load("malbouffe.RData")
data[,2:7]<-apply(data[,2:7],2,as.factor) # variables qualitatives
data<-DataExplorer::update_columns(data, c(1,8:11), as.numeric)
summary(data)
```

```{r}
library(MASS)
library(SignifReg)
library(leaps)
library(gtools)
```


# Statistiques descriptives

```{r}
DataExplorer::plot_intro(data)

knitr::kable(
  data[1:3, 2:7], caption = 'Variables qualitatives (colonnes 2 à 7) de malbouffe: 3 premières lignes'
)
knitr::kable(
  data[1:3, c(1,8:11)], caption = 'Variables quantitatives (colonnes 8 a 11) de malbouffe: 3 premières lignes'
)
```

## Transformation en factor:

```{r}
for(j in 2:7){
  data[,j]<-as.factor(data[,j])
}
```

## Transformation en numeric:

```{r}
# Pareil pour s'assurer que les quanti sont déclarées quanti. 
# Probablement as.character() innecessaire ici. Parfois cela permet d'éviter des problèmes.
for(j in c(1,8:11)){
  data[,j]<-as.numeric(as.character(data[,j]))
}
```

### Création de variables quali à partir de quanti:

```{r}
data$corpulence<- cut(data$imc,
                      c(0, 16.5, 18.5, 25, 30, 35, 40, 100),
                      include.lowest = TRUE,
                      labels = c("denutrition", "maigreur", "normale", "surpoids",
                                 "obesite moderee", "obesite sever", "obesite morbide"))
table(data$corpulence)
data$obesite<- cut(data$imc, c(16.5, 25, 100), include.lowest = TRUE,
                   labels = c("maigreur ou normale", "surpoids ou obesite"))
table(data$obesite)

# Valeurs manquantes
DataExplorer::plot_missing(data)

```

## Ré-ordonner les colonnes pour avoir les quanti d'un coté et les quali de l'autre:

```{r}
data<-data[,c(2:7,12:13,1,8:11)]
quali<-1:8
quanti<-9:13
summary(data)
```

## Univariable

```{r}
library(Hmisc)
options(digits=1)
Hmisc::describe(data)
```

### Quanti

```{r}
#Histogramme, d'abord 1 à 1 :
hist(data[,9],col="blue", xlab = names(data)[9],main = "")
#ou
#hist(data$age,col="blue", xlab = "Age",main = "")
#Ou toutes les quanti d'un seul coup avec une boucle :
#partage de la zone de graphique en 1 ligne et 5 colonnes
par(mfrow=c(1,5))
#boucle qui fait courrir l'indice j dans quanti, soit l'objet qui contient les colonnes des quanti
for (j in quanti) {
  hist(data[,j],col="blue", xlab = names(data)[j],main = "")
}
#ou avec le package dataExplorer
DataExplorer::plot_histogram(data)

```

### Quali

```{r}
#pareil pour les quali avec la fonction barplot (des effectifs donc on fait table)
par(mfrow=c(2,4))
for (j in quali) {
  barplot(table(data[,j]),col="red", main = names(data)[j])
}
#ou avec DataExplorer
DataExplorer::plot_bar(data)
```

## Bivariable

## QUALI VS QUALI
```{r}
#Pour croiser toutes les quali d'un coup, on utilise 2 boucles imbriquées
#Pour ne pas refaire 2 fois les mêmes (d'abord sexe x revenus, puis revenus x sexe, par ex.)
#on rajoute une condition avec if: on fera ainsi le graphique uniquement si l'indice du 1er est supérieur au 2ème 
par(mfrow=c(2,4))
for(j in quali){
  for(i in quali){
    if(i>j){
      epiDisplay::tabpct(data[,j],data[,i],decimal = 0, percent = "col",
                         graph = TRUE, xlab=names(data)[j],
                         ylab =names(data)[i], main="")
    }}}


#Obesite VS les autres QUALI
#la fonction summary de Hmisc permet de résumer obesite en fonction du sexe à partir de la fonction indiquée dans fun (ici table) 
#obesite a des valeurs manquantes, il faut les exclure
#summary(obesite~sexe,data=data[!is.na(data$obesite),],fun=table)
```

### Test Khi2
```{r}
#Variable par variable
#vérification des conditions d'application :
chisq.test(data$obesite,data$sexe)$expected
chisq.test(data$obesite,data$sexe,correct = FALSE) #sans correction de Yates puisque grands échantillons

#Toutes les variables VS obésité d'un coup avec apply
apply(data[,quali],2,function(x) chisq.test(data$obesite,x))

#Avec DataExplorer
DataExplorer::plot_bar(data, by ="sexe")
```

## QUANTI VS QUANTI
```{r}
par(mfrow=c(1,1))
#Matrice de corrélations
cor(data[,quanti])
library(ggplot2)
library(ggcorrplot)
# Représentation graphique de la matrice de corrélations
ggcorrplot(cor(data[,quanti]))

#-------------------------
# Test sur le coefficient de corrélation:
#-------------------------
# Paire par paire
cor.test(data$exercice,data$malbouffe)
#Toutes les variables VS malbouffe d'un coup avec apply
apply(data[,quanti],2,function(x) cor.test(data$malbouffe,x))


par(mfrow=c(1,2))
#IMC VS score
plot(imc~malbouffe,data=data,pch = 20,col="blue")
#IMC VS exercice
plot(imc~exercice,data=data,pch = 20,col="blue")

#Avec DataExplorer
library(DataExplorer)
plot_correlation(data, cor_args = list("use" = "pairwise.complete.obs"), type="c")
plot_scatterplot(split_columns(data)$continuous, by = "malbouffe")
```
## QUANTI VS QUALI
```{r}
#Obesite VS les QUANTI
#Variable par variable 
#Hmisc::summarize(data$malbouffe, by=data$obesite, FUN=smean.sd, stat.name=c('Mean','SD')) #function smean.sd = moyenne et écart-type
#Hmisc::summarize(data$malbouffe, by=data$obesite, quantile, probs=c(.5,.25,.75),stat.name=c('Q2','Q1','Q3')) #function quantiles

#Toutes les variables d'un coup 
apply(data[,quanti],2,function(x) Hmisc::summarize(x, by=data$obesite, FUN=smean.sd))
par(mfrow=c(1,1))

#Boxplot variable par variable
boxplot(data$exercice~data$obesite, col="blue")
```

```{r}
#-------------------------
#t.test avec variances differentes si p-value du test variances < 10% (manque normalité, mais ici grandes bases de données)
#-------------------------
var.test(data$exercice~data$obesite)$p.value
t.test(data$exercice~data$obesite, var.equal=var.test(data$exercice~data$obesite)$p.value>0.1)

#revenus VS les QUANTI
apply(data[,quanti],2,function(x) Hmisc::summarize(x, by=data$revenus, FUN=smean.sd))
par(mfrow=c(1,1))
boxplot(data$exercice~data$revenus, col="blue")
```

## anova (manque normalité, mais ici grandes bases de données)
```{r}
bartlett.test(data$exercice~data$revenus)$p.value
summary(aov(data$exercice~data$revenus)) #aov analysis of variance mais supposant variances égales
oneway.test(malbouffe ~ revenus, data=data) # Welch test: sans supposer que les variances sont égales

#Avec DataExplorer
DataExplorer::plot_boxplot(data, by ="sexe")
```

## Régression linéaire

```{r}
M1<-lm(malbouffe~sexe,data=data)
summary(M1)
confint(M1, level=0.95)
#Résumé avec epiDisplay 
#library(epiDisplay)
epiDisplay::regress.display(M1)
#Résumé avec gtsummary 
library(gtsummary)
tbl_regression(M1) #test t
tbl_regression(M1) %>% add_global_p() #test F
```


```{r}
M2<-lm(malbouffe~exercice,data=data)
summary(M2)
confint(M2, level=0.95)
#Résumé avec epiDisplay 
epiDisplay::regress.display(M2)
#Résumé avec gtsummary 
tbl_regression(M2)
tbl_regression(M2) %>% add_global_p()
```

```{r}
M3<-lm(malbouffe~revenus,data=data)
summary(M3)
confint(M3, level=0.95)
#Résumé avec epiDisplay 
epiDisplay::regress.display(M3)
#Résumé avec gtsummary 
tbl_regression(M3)
tbl_regression(M3) %>% add_global_p()
```

```{r}
M4<-lm(malbouffe~revenus+sexe,data=data)
summary(M4)
confint(M4, level=0.95)
#comparaison des modèles 4 et 1 : apport global des revenus au modèle qui contient déjà le sexe
anova(M1, M4)
#Résumé avec epiDisplay 
epiDisplay::regress.display(M4)
#Résumé avec gtsummary 
tbl_regression(M4)
tbl_regression(M4) %>% add_global_p()
```



# Modèles de régression

## BACKWARD avec AIC

```{r}
load("malbouffe.RData")
```


```{r}
model_complet <- lm(formula = malbouffe ~ ., data = data)
step.b.AIC <- step(model_complet, direction = "backward", data = data)
summary(step.b.AIC)
```

## BACKWARD avec p-value

```{r}
fullmodel <- lm(malbouffe ~ ., data = data)
step.b.pvalue <- SignifReg(fit = fullmodel, scope = list(lower = ~1, upper = fullmodel), alpha = 0.05, direction = "backward", criterion = "p-value", adjust.method = "none", trace = FALSE)
summary(step.b.pvalue)
```

## FORWARD avec AIC

```{r}
model_vide <- lm(formula = malbouffe ~ 1, data = data)
step.f.AIC <- stepAIC(model_vide, scope = list(upper = model_complet, lower = model_vide), direction = "forward", data = data)
summary(step.f.AIC)
```

## FORWARD avec p-value

```{r}
nullmodel <- lm(malbouffe ~ 1, data = data)
step.f.pvalue <- SignifReg(fit = nullmodel, scope = list(upper = fullmodel, lower = nullmodel), alpha = 0.05, direction = "forward", criterion = "p-value", adjust.method = "none", trace = FALSE)
summary(step.f.pvalue)
```

## ALL SUBSETS avec Cross Validation (CV)

```{r}
#Convertir les variables qualitatives en numériques : 
#utiliser la fonction model.matrix() pour transformer automatiquement les facteurs en variables indicatrices (dummy variables)

# Nombre d'observations
n <- dim(data)[1]

# Séparer les variables explicatives (x) et la variable réponse (y)
x <- data[, -which(colnames(data) == "malbouffe")]
y <- as.matrix(data[,"malbouffe"], nrow=n)
colnames(y) <- colnames(data)[colnames(data) == "malbouffe"]
p <- dim(x)[2]

# Fonction pour concaténer des chaînes
"%+%" <- function(x, y) paste(x, y, sep="")

# Générer tous les sous-ensembles de variables
models <- cbind(combinations(p, 1, 1:p), array(0, dim = c(p, p-1)))
for (i in 2:p) {
  nc <- dim(combinations(p, i, 1:p))[1]
  models <- rbind(models, cbind(combinations(p, i, 1:p), array(0, dim = c(nc, p-i))))
}

# Nombre de folds pour la cross-validation
folds <- 10
all.folds <- split(sample(1:n), rep(1:folds, length = n))
error <- matrix(NA, ncol = 1, nrow = folds)
errorCV <- matrix(NA, ncol = 1, nrow = dim(models)[1])

# Boucle sur tous les sous-ensembles possibles de variables
for (i in 1:dim(models)[1]){
  variables <- models[i, which(models[i,] > 0)]
  X <- x[, variables]  # Sélection des colonnes correspondantes
  
  # Gestion du cas où il n'y a qu'une seule variable
  if (length(variables) == 1){
    X <- as.data.frame(X)
    colnames(X) <- colnames(x)[variables]
  }
  
  # Utilisation de model.matrix pour transformer les variables qualitatives en dummies
  X_full <- model.matrix(~ ., data = as.data.frame(X))[,-1]  # Supprimer l'intercept
  
  # Transformer X_full en matrice si nécessaire (pour éviter les erreurs)
  if (is.null(dim(X_full))) {
    X_full <- matrix(X_full, ncol = 1)
  }
  
  model_i <- as.formula(paste("malbouffe ~", paste(colnames(X), collapse = "+")))
  
  for (j in seq(folds)){
    #**********
    # Estimation
    #**********
    omit <- all.folds[[j]]
    
    # Apprentissage du modèle sur les données d'entraînement (sans les observations omises)
    beta <- lm(model_i, data = data[-omit,])$coefficients
    
    #**********
    # Test
    #**********
    # Appliquer les mêmes transformations sur les données de test (omises)
    X_test <- as.matrix(X_full[omit,])  # Transformation des variables qualitatives en matrice
    if (is.null(dim(X_test))) {
      X_test <- matrix(X_test, ncol = 1)  # Assurer que X_test reste une matrice
    }
    
    # Calcul des prédictions
    Y_hat <- beta[1] + X_test %*% beta[-1]
    
    # Calcul de l'erreur
    error[j] <- t(Y_hat - y[omit,]) %*% (Y_hat - y[omit,]) / length(omit)
  }
  
  # Calcul de l'erreur moyenne sur tous les folds
  errorCV[i] <- mean(error)
}

# Afficher le modèle qui minimise l'erreur de validation croisée
best_model <- models[which.min(errorCV), ]
colnames(x)[best_model]
```

## ALL SUBSETS avec AIC

```{r}
# Packages nécessaires
library(combinat)  # Pour générer les combinaisons
n <- dim(data)[1] # Nombre d'observations
x <- data[, -which(colnames(data) == "malbouffe")]  # Variables explicatives
y <- data[,"malbouffe"]  # Variable dépendante
p <- dim(x)[2]  # Nombre de variables explicatives

# Génération des combinaisons de modèles
models <- list()

for (i in 1:p) {
  models[[i]] <- combinations(p, i, 1:p)  # Tous les sous-ensembles de taille i
}

# Fonction pour calculer l'AIC pour chaque sous-ensemble de variables
get_AIC <- function(variables, data) {
  X <- data[, variables]
  
  if (length(variables) <= 1) {
    X <- as.matrix(X)
    colnames(X) <- colnames(data)[variables]
  }
  
  # Construction du modèle et calcul de l'AIC
  model_formula <- as.formula(paste("malbouffe ~", paste(colnames(X), collapse = "+")))
  fit <- lm(model_formula, data = data)
  return(AIC(fit))
}

# Liste pour stocker les AIC et les modèles
results <- data.frame(AIC = numeric(), Model = character())

# Boucle sur toutes les combinaisons possibles de variables
for (i in 1:p) {
  for (j in 1:nrow(models[[i]])) {
    variables <- models[[i]][j, ]
    
    # Calcul de l'AIC pour ce sous-ensemble de variables
    current_AIC <- get_AIC(variables, data)
    
    # Sauvegarde du résultat
    results <- rbind(results, data.frame(AIC = current_AIC, 
                                         Model = paste(colnames(x)[variables], collapse = "+")))
  }
}

# Trouver le modèle avec l'AIC minimum
best_model <- results[which.min(results$AIC), ]
print(best_model)
```


## LASSO (alpha=1) et RIDGE (alpha=0) --> Pour RIDGE C'est la même chose qu'avec LASSO, il suffit de changer alpha=1 à alpha=0

On sait que la variable malbouffe a été créée en fonction de fruits donc c'est pas utile de garder la variable puisqu'on sait déjà que ça a un impact. Il vaut mieux laisser la place aux autres variables pour qu'on puisse voir comment elles impactent mallbouffe
En gros quand on affiche le LASSO et qu'on voit que qu'une variable est très éloignée des autres dès le début, il vaut mieux la supprimer parce que on sait déjà qu'elle aura un impact significatif.

### LASSO

```{r}

# Créer une matrice de variables explicatives en transformant automatiquement les facteurs en variables indicatrices
# model.matrix génère une matrice de variables indicatrices pour les colonnes qualitatives
X <- model.matrix(~ . - 1, data = data[, c(1:7, 10:11)]) # Le -1 permet de ne pas inclure l'intercept car il sera inclus dans le LASSO

# Extraire la variable dépendante
Y <- data[,9]

library(glmnet)

# Ajustement du modèle LASSO
fit.lasso <- glmnet(x = X, y = Y, family = 'gaussian', alpha = 1, nlambda = 100, 
                    standardize = TRUE, intercept = TRUE)

# Visualisation des coefficients
plot(fit.lasso, label = TRUE)

# Obtenir les noms des variables indicatrices créées par model.matrix
variable.names <- colnames(X)

# Ajouter la légende avec les noms des variables
legend("topleft", legend = variable.names, col = 1:ncol(X), lty = 1, cex = 0.8)
```

```{r}
#X <- as.matrix(X) # Conversion de X en matrice, car glmnet nécessite une matrice pour les régressions pénalisées

# Ajustement du modèle LASSO avec validation croisée
# cv.glmnet applique une validation croisée à 10 plis (nfold = 10) pour choisir la meilleure valeur de lambda
fit.cv.lasso <- cv.glmnet(x = X, y = Y, family = 'gaussian', alpha = 1, nlambda = 100, 
                          nfold = 10, standardize = TRUE, intercept = TRUE)

# Affichage des erreurs estimées par validation croisée en fonction des valeurs de lambda
# Cela permet de visualiser la courbe de l'erreur de validation croisée pour déterminer la valeur optimale de lambda
plot(fit.cv.lasso)

# Résumé du modèle LASSO avec validation croisée
# Fournit des détails sur les performances et les coefficients sélectionnés pour le lambda optimal
summary(fit.cv.lasso)
```

```{r}
# Récupération de la valeur optimale de lambda minimisant l'erreur de validation croisée
lamb.opt.cvm <- fit.cv.lasso$lambda.min 

# Extraction des coefficients du modèle LASSO correspondant à lambda optimal
coef.fit <- predict(fit.cv.lasso, s = lamb.opt.cvm, type = "coefficients")

# Récupération de la valeur de lambda pour la règle "1-SE" (un compromis entre biais et variance)
lamb.opt.1se <- fit.cv.lasso$lambda.1se

# Extraction des coefficients du modèle LASSO correspondant à lambda pour la règle 1-SE
coef.fit <- predict(fit.cv.lasso, s = lamb.opt.1se, type = "coefficients")

# Affichage de la courbe des erreurs de validation croisée par rapport à lambda
plot(fit.cv.lasso)

# Ajout d'une ligne bleue indiquant la valeur de lambda qui minimise l'erreur de validation croisée
abline(v = log(lamb.opt.cvm), col = 'blue', lty = 'solid', lwd = 2)

# Ajout d'une ligne verte indiquant la valeur de lambda selon la règle 1-SE
abline(v = log(lamb.opt.1se), col = 'green', lty = 'solid', lwd = 2)
```

```{r}
# Validation croisée pour sélectionner la meilleure valeur de lambda
fit.cv.lasso <- cv.glmnet(x = X, y = Y, family = 'gaussian', alpha = 1, nlambda = 100, 
                          standardize = TRUE, intercept = TRUE)

# Utiliser la valeur de lambda qui minimise l'erreur de validation croisée
lamb.opt.cvm <- fit.cv.lasso$lambda.min

# Obtenir les coefficients pour lambda.min
coef.min <- predict(fit.cv.lasso, s = lamb.opt.cvm, type = "coefficients")

# Afficher les variables sélectionnées (celles avec des coefficients non nuls)
selected.variables.min <- rownames(coef.min)[which(coef.min != 0)]
selected.variables.min
```

```{r}
# Utiliser la valeur de lambda selon la règle "1-SE"
lamb.opt.1se <- fit.cv.lasso$lambda.1se 

# Obtenir les coefficients pour lambda.1se
coef.1se <- predict(fit.cv.lasso, s = lamb.opt.1se, type = "coefficients")

# Afficher les variables sélectionnées (celles avec des coefficients non nuls)
selected.variables.1se <- rownames(coef.1se)[which(coef.1se != 0)]
selected.variables.1se
```


### Quels variables ont été sélectionnées par LASSO : 

Pour voir quelles variables ont été sélectionnées par votre modèle LASSO, vous pouvez examiner les coefficients non nuls après avoir ajusté le modèle avec la valeur de **lambda** choisie (soit `lambda.min` pour la valeur optimale, soit `lambda.1se` pour un modèle plus simple).

Voici les étapes et le code pour extraire les variables sélectionnées :

1. **Utilisation de la valeur optimale de lambda** :
   - `lambda.min` : La valeur de lambda qui minimise l'erreur de validation croisée.
   - `lambda.1se` : Une valeur de lambda légèrement plus élevée qui privilégie la simplicité du modèle (sélectionne moins de variables).

2. **Extraire les coefficients non nuls** :
   Utilisez la fonction `predict` avec l'option `type = "coefficients"` pour obtenir les coefficients associés à la valeur de **lambda** optimale.


#### Explication :
- `which(coef.min != 0)` renvoie les indices des coefficients qui ne sont pas nuls, c'est-à-dire les variables sélectionnées par LASSO.
- Ensuite, `rownames(coef.min)` vous donne les noms des variables correspondantes à ces coefficients.

Cela vous permet de voir clairement quelles variables explicatives ont été retenues par le modèle LASSO et contribuent réellement à la prédiction de la variable dépendante (**malbouffe**).

## RIDGE
```{r}
library(glmnet) # Chargement du package glmnet pour les régressions pénalisées (LASSO et RIDGE)

# Ajustement du modèle LASSO
# alpha = 0 spécifie que l'on utilise la pénalisation RIDGE
# standardize = TRUE normalise les variables explicatives avant l'ajustement
# intercept = TRUE inclut une constante (intercept) dans le modèle
fit.ridge <- glmnet(x = X, y = Y, family = 'gaussian', alpha = 0, nlambda = 100, 
                    standardize = TRUE, intercept = TRUE)

# Visualisation des coefficients beta en fonction de la valeur de lambda
# Chaque ligne correspond à un coefficient, et les variables sont sélectionnées progressivement avec la diminution de lambda
plot(fit.ridge, label = TRUE)

# Résumé du modèle LASSO : permet d'obtenir des informations sur l'ajustement
summary(fit.ridge)
```

```{r}
X<-as.matrix(X)
fit.cv.ridge<-cv.glmnet(x=X,y=Y,family='gaussian',alpha=0,nblambda=100,nfold=10,standardize = TRUE,intercept=TRUE)
#estimation de beta et la sélection de variables
plot(fit.cv.ridge) # on plot l'estimation de l'erreur par validation croisée
summary(fit.cv.ridge)
```

```{r}
lamb.opt.cvm <- fit.cv.ridge$lambda.min
coef.fit <- predict(fit.cv.ridge, s=lamb.opt.cvm, type = "coefficient")
lamb.opt.1se <- fit.cv.ridge$lambda.1se
coef.fit <- predict(fit.cv.ridge, s=lamb.opt.1se, type = "coefficient")
plot(fit.cv.ridge)
abline(v = log(lamb.opt.cvm),col='blue',lty='solid',lwd=2)
abline(v = log(lamb.opt.1se),col='green',lty='solid',lwd=2)
```

```{r}
# Utiliser la valeur de lambda qui minimise l'erreur de validation croisée
lamb.opt.cvm <- fit.cv.ridge$lambda.min 

# Obtenir les coefficients pour lambda.min
coef.min <- predict(fit.cv.ridge, s = lamb.opt.cvm, type = "coefficients")

# Afficher les variables sélectionnées (celles avec des coefficients non nuls)
selected.variables.min <- rownames(coef.min)[which(coef.min != 0)]
selected.variables.min
```

#### Si vous voulez utiliser la règle 1-SE pour une sélection plus conservatrice :

```{r}
# Utiliser la valeur de lambda selon la règle "1-SE"
lamb.opt.1se <- fit.cv.ridge$lambda.1se 

# Obtenir les coefficients pour lambda.1se
coef.1se <- predict(fit.cv.ridge, s = lamb.opt.1se, type = "coefficients")

# Afficher les variables sélectionnées (celles avec des coefficients non nuls)
selected.variables.1se <- rownames(coef.1se)[which(coef.1se != 0)]
selected.variables.1se
```